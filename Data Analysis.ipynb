{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext, SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+-------+-----+----+-----------+\n",
      "|                Name|              Author|User Rating|Reviews|Price|Year|      Genre|\n",
      "+--------------------+--------------------+-----------+-------+-----+----+-----------+\n",
      "|10-Day Green Smoo...|            JJ Smith|        4.7|  17350|    8|2016|Non Fiction|\n",
      "|   11/22/63: A Novel|        Stephen King|        4.6|   2052|   22|2011|    Fiction|\n",
      "|12 Rules for Life...|  Jordan B. Peterson|        4.7|  18979|   15|2018|Non Fiction|\n",
      "|1984 (Signet Clas...|       George Orwell|        4.7|  21424|    6|2017|    Fiction|\n",
      "|5,000 Awesome Fac...|National Geograph...|        4.8|   7665|   12|2019|Non Fiction|\n",
      "+--------------------+--------------------+-----------+-------+-----+----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('bestsellers\\ with\\ categories.csv', inferSchema=True, header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Author: string (nullable = true)\n",
      " |-- User Rating: double (nullable = true)\n",
      " |-- Reviews: integer (nullable = true)\n",
      " |-- Price: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Genre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#shows us the information about all the columns which are there in the dataset.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count the no of records. The code implemented for this shown below\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='10-Day Green Smoothie Cleanse', Author='JJ Smith', User Rating=4.7, Reviews=17350, Price=8, Year=2016, Genre='Non Fiction')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. Display record in different ways\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------------+-------------------+------------------+------------------+------------------+-----------+\n",
      "|summary|                Name|          Author|        User Rating|           Reviews|             Price|              Year|      Genre|\n",
      "+-------+--------------------+----------------+-------------------+------------------+------------------+------------------+-----------+\n",
      "|  count|                 550|             550|                550|               550|               550|               550|        550|\n",
      "|   mean|                null|            null|  4.618363636363641|11953.281818181818|              13.1|            2014.0|       null|\n",
      "| stddev|                null|            null|0.22698036502519656|11731.132017431892|10.842261978422364|3.1651563841692782|       null|\n",
      "|    min|\"The Plant Parado...|Abraham Verghese|                3.3|                37|                 0|              2009|    Fiction|\n",
      "|    max|You Are a Badass:...|    Zhi Gang Sha|                4.9|             87841|               105|              2019|Non Fiction|\n",
      "+-------+--------------------+----------------+-------------------+------------------+------------------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#used to display the statistical properties of all the columns in the dataset.\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             Price|\n",
      "+-------+------------------+\n",
      "|  count|               550|\n",
      "|   mean|              13.1|\n",
      "| stddev|10.842261978422364|\n",
      "|    min|                 0|\n",
      "|    max|               105|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use describe function column-wise also.\n",
    "df.describe('Price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of duplication\n",
    "df =df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name \t with '0' values:  0.0\n",
      "Author \t with '0' values:  0.0\n",
      "User Rating \t with '0' values:  0.0\n",
      "Reviews \t with '0' values:  0.0\n",
      "Price \t with '0' values:  0.02181818181818182\n",
      "Year \t with '0' values:  0.0\n",
      "Genre \t with '0' values:  0.0\n"
     ]
    }
   ],
   "source": [
    "#Checking missing values\n",
    "total =df.count()\n",
    "for col in df.columns:\n",
    "    df_filter =(df.filter(df[col]==\"0\")).count()\n",
    "    percen_filter = df_filter/total\n",
    "    print(col, \"\\t\",\"with '0' values: \",percen_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Price']\n"
     ]
    }
   ],
   "source": [
    "#printing missing coloumns\n",
    "missing_col =[]\n",
    "for col in df.columns:\n",
    "    df_filter =(df.filter(df[col]==\"0\")).count()\n",
    "    if(df_filter > 0):\n",
    "        missing_col.append(col)\n",
    "print(missing_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing mean values for numerical coloumns\n",
    "#for null values with 0, it will be filled for the missing numerical coloumns.\n",
    "df =df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the avg of all numeric columns\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "def find_columns_fill_mean(df,num_col, verbose=False):\n",
    "    col_with_mean=[]\n",
    "    for col in num_col:\n",
    "        mean_value =df.select(avg(df[col]))\n",
    "        avg_col =mean_value.columns[0]\n",
    "        result = mean_value.rdd.map(lambda row : row[avg_col]).collect()\n",
    "        \n",
    "        if (verbose==True): print(mean_value.columns[0], \"\\t\", result[0])\n",
    "        col_with_mean.append([col, result[0]])\n",
    "    return col_with_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre \t mean values:  [['Price', 13.1]]\n"
     ]
    }
   ],
   "source": [
    "print(col, \"\\t\", \"mean values: \", find_columns_fill_mean(df,missing_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling missing values for mean\n",
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "def fill_missing_value_with_mean(df, numeric_cols):\n",
    "    col_mean = mean_of_pyspark_columns(df, numeric_cols)\n",
    "    \n",
    "    for col, mean in col_mean:\n",
    "        df = df.withColumn(col, when(df[col]==0, lit(mean)).otherwise(df[col]))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_filter1 = fill_missing_value_with_mean(df, missing_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_filter1 =df_mean_filter1.repartition(1)\n",
    "df_mean_filter1.write.csv('bestsellers with categories.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
